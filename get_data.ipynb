{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a9e706-d298-44ec-8d96-aa7f8465fecb",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "> This notebook retrieves all data necessary from the API and stores it in json files for visualization in downstream tasks. You can find the visualization takss in `visualize.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "3531e50f-bc8e-495b-bd9d-1487f0bb509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse, collections\n",
    "\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d0a71-cc9c-4996-98f8-12529c1094c9",
   "metadata": {},
   "source": [
    "### Constants \n",
    "\n",
    "> These variables will stay the same for all functions used in this notebook. They are used in our functions for querying data but remain the same throughout. If you choose to reproduce/build on this work then you will need to change a few fields. These fields, `Request Headers`, `ARTICLE_PAGEVIEWS_PARAMS_DESKTOP`, `ARTICLE_PAGEVIEWS_PARAMS_MOBILE_APP`, and `ARTICLE_PAGEVIEWS_PARAMS_MOBILE_WEB` may need to be changed. The User Agent will need to be changed to your name and affiliation. The start and end keys will need to be changed to the dates that you would like to access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7120fb86-2422-4823-b9d9-d0c4fc683fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<braj1@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_DESKTOP = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",    \n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",       \n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",  # July 1st Hour 00 of 2015\n",
    "    \"end\":         \"2022090200\"   # Oct 1st Hour 00 of 2022\n",
    "}\n",
    "\n",
    "ARTICLE_PAGEVIEWS_PARAMS_MOBILE_APP = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-app\",  \n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",         \n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",  # July 1st Hour 00 of 2015\n",
    "    \"end\":         \"2022090200\"   # Oct 1st Hour 00 of 2022\n",
    "}\n",
    "\n",
    "ARTICLE_PAGEVIEWS_PARAMS_MOBILE_WEB = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-web\",  \n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",            \n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",  # July 1st Hour 00 of 2015\n",
    "    \"end\":         \"2022090200\"   # Oct 1st Hour 00 of 2022\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5cba1-557c-4915-bfca-c9a0528e5ad4",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "> These are the functions we use for making requests to the API. The function `request_pageviews_per_article` allows us to query the endpoint for an article for monthly views.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1deb0527-27b2-4db6-ba6c-ecb8096c174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  request_type = None,\n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  headers = REQUEST_HEADERS) -> str:\n",
    "    '''Retrieves JSON response from Wikpedia API given the title of the article and parameters (saved in constants above)'''\n",
    "    if request_type.lower() not in ['desktop', 'mobile-app', 'mobile-web']:\n",
    "        print('Please pass in request template to be desktop, mobile-app, or mobile-user')\n",
    "        return None\n",
    "    \n",
    "    if request_type.lower() == 'mobile-web':\n",
    "        request_template = ARTICLE_PAGEVIEWS_PARAMS_MOBILE_WEB\n",
    "    elif request_type.lower() == 'mobile-app':\n",
    "        request_template = ARTICLE_PAGEVIEWS_PARAMS_MOBILE_APP\n",
    "    else:\n",
    "        request_template = ARTICLE_PAGEVIEWS_PARAMS_DESKTOP\n",
    "    \n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def get_dinosaur_article_names(input_csv_file_location) -> list:\n",
    "    '''Read in a csv to get all dinosaur article names'''\n",
    "    # Make sure we are actually passing in an input csv\n",
    "    if not input_csv_file_location: return None\n",
    "    \n",
    "    df = pd.read_csv(input_csv_file_location)\n",
    "    \n",
    "    if 'name' not in df.columns:\n",
    "        return None # could not find name of articles in that input csv.\n",
    "    \n",
    "    names = list(df['name'].values) # convert to list of names from numpy array\n",
    "                                             \n",
    "    return names\n",
    "\n",
    "def generate_json_for_articles(article_names, request_type, save_name) -> str:\n",
    "    ''' \n",
    "    This is a helper function to generate dataframes to be aggregated before\n",
    "    the json step. This takes only two request types, desktop and mobile. Mobile will\n",
    "    group mobile-app and mobile-web together.\n",
    "    '''\n",
    "    if request_type.lower() not in ['desktop', 'mobile', 'cumulative']:\n",
    "        print('Please pass in request template to be desktop, mobile-app, or mobile-user')\n",
    "        return None\n",
    "    \n",
    "    per_article_map = collections.defaultdict(str)\n",
    "    \n",
    "    for name in article_names:\n",
    "        try:\n",
    "            if request_type == 'desktop':\n",
    "                desktop_views = request_pageviews_per_article(article_title=name, request_type='desktop')\n",
    "                for item in desktop_views['items']:\n",
    "                    del item['access']\n",
    "\n",
    "                per_article_map[name] = desktop_views['items']\n",
    "            elif request_type == 'mobile':\n",
    "                web_views = request_pageviews_per_article(article_title=name, request_type='mobile-app')\n",
    "                app_views = request_pageviews_per_article(article_title=name, request_type='mobile-web')\n",
    "                per_article_map[name] = _sum_article_views(name_of_article=name, type_views=[web_views, app_views])\n",
    "            else:\n",
    "                desktop_views = request_pageviews_per_article(article_title=name, request_type='desktop')\n",
    "                web_views = request_pageviews_per_article(article_title=name, request_type='mobile-app')\n",
    "                app_views = request_pageviews_per_article(article_title=name, request_type='mobile-web')\n",
    "                per_article_map[name] = _sum_article_views(name_of_article=name, type_views=[desktop_views, app_views, web_views])\n",
    "        except Exception as e:\n",
    "            # print(views)\n",
    "            print(f'Error at article: {name}, error; {e}')\n",
    "    \n",
    "    article_json_obj = json.dumps(per_article_map, indent = 4) \n",
    "    \n",
    "    with open(f\"{save_name}.json\", \"w+\") as outfile:\n",
    "        outfile.write(article_json_obj)\n",
    "        \n",
    "    print(f'Wrote JSON to file: {save_name}.json')\n",
    "    return article_json_obj\n",
    "\n",
    "def _sum_article_views(name_of_article, type_views=[]):\n",
    "    '''Helper function to retrieve and sum all articles of similar type between different access types'''\n",
    "    if len(type_views) == 0:\n",
    "        print('Plase pass in json objects of num views')\n",
    "        return None\n",
    "    try:\n",
    "        combined_view = collections.defaultdict(int)\n",
    "        \n",
    "        for views in type_views:\n",
    "            for view in views['items']:\n",
    "                ts, num_views = view['timestamp'], view['views']\n",
    "                combined_view[ts] += num_views\n",
    "\n",
    "        result_output = []\n",
    "\n",
    "        for ts,num_views in combined_view.items():\n",
    "            result_output.append({\n",
    "                'project': 'en.wikipedia',\n",
    "                'article': name_of_article,\n",
    "                'granularity': 'monthly',\n",
    "                'timestamp': ts,\n",
    "                'agent': 'user',\n",
    "                'views': num_views\n",
    "            })\n",
    "            \n",
    "        return result_output\n",
    "    except Exception as e:\n",
    "        print(f'error with article: {name_of_article} with error : {e}')\n",
    "        return None\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f498c-94ff-4047-9abf-599d69a9b974",
   "metadata": {},
   "source": [
    "### Get the output for each name within the dinosaur csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4fdba8a2-459f-49d9-888f-a185eb34c4d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coelosaurus antiquus',\n",
       " 'Aachenosaurus',\n",
       " 'Aardonyx',\n",
       " 'Abdarainurus',\n",
       " 'Abditosaurus',\n",
       " 'Abelisaurus',\n",
       " 'Abrictosaurus',\n",
       " 'Abrosaurus',\n",
       " 'Abydosaurus',\n",
       " 'Acantholipan']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names = get_dinosaur_article_names('./dinosaur_genera.cleaned.SEPT.2022 - dinosaur_genera.cleaned.SEPT.2022.csv.csv')\n",
    "article_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdc7ae-2549-4328-9a6c-8d1307c1de2c",
   "metadata": {},
   "source": [
    "### Make Requests to Wikipedia to generate JSON files\n",
    "\n",
    "> Here we make requests to the Wikipedia API using the `generate_json_for_articles` method. This method will sum all json outputs for an article name if there are multiple JSON requests using the `_sum_article_views` helper function. Finally, they will save them into the json file name that we pass in. Below, we pass in the same `dino_monthly_mobile_<201507>-<202209>`, `dino_monthly_desktop_<201507>-<202209>`, `dino_monthly_cumulative_<201507>-<202209>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d9666b98-6bb7-4a6b-a599-1b043d6e4ffd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with article: Elemgasem with error : 'items'\n",
      "error with article: Tuebingosaurus with error : 'items'\n",
      "Wrote JSON to file: dino_monthly_mobile_<201507>-<202209>.json\n"
     ]
    }
   ],
   "source": [
    "mobile_articles = generate_json_for_articles(article_names, 'mobile', 'data/dino_monthly_mobile_<201507>-<202209>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "0b4c5a93-5a40-49c0-8b63-255bf87b0f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at article: Elemgasem, error; 'items'\n",
      "Error at article: Tuebingosaurus, error; 'items'\n",
      "Wrote JSON to file: dino_monthly_desktop_<201507>-<202209>.json\n"
     ]
    }
   ],
   "source": [
    "desktop_articles = generate_json_for_articles(article_names, 'desktop', 'data/dino_monthly_desktop_<201507>-<202209>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f7357826-1e97-452d-96d1-85e289e4334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with article: Elemgasem with error : 'items'\n",
      "error with article: Tuebingosaurus with error : 'items'\n",
      "Wrote JSON to file: dino_monthly_cumulative_<201507>-<202209>.json\n"
     ]
    }
   ],
   "source": [
    "cumulative_articles = generate_json_for_articles(article_names, 'cumulative', 'data/dino_monthly_cumulative_<201507>-<202209>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42632c3c-72b0-469b-a4b5-31f7e6afef52",
   "metadata": {},
   "source": [
    "> Looks like Elemgasem/Tuebingosaurus are invalid API request to Wikpedia API so they will not be in our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
